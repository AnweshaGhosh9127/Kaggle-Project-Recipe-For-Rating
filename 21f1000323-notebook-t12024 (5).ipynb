{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":67079,"databundleVersionId":7452256,"sourceType":"competition"}],"dockerImageVersionId":30635,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-10T14:42:36.305025Z","iopub.execute_input":"2024-04-10T14:42:36.305473Z","iopub.status.idle":"2024-04-10T14:42:36.315746Z","shell.execute_reply.started":"2024-04-10T14:42:36.305437Z","shell.execute_reply":"2024-04-10T14:42:36.314466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Load basic libraries**","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:36.318436Z","iopub.execute_input":"2024-04-10T14:42:36.319171Z","iopub.status.idle":"2024-04-10T14:42:36.327157Z","shell.execute_reply.started":"2024-04-10T14:42:36.319129Z","shell.execute_reply":"2024-04-10T14:42:36.325953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.metrics import accuracy_score","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:36.328522Z","iopub.execute_input":"2024-04-10T14:42:36.329303Z","iopub.status.idle":"2024-04-10T14:42:36.338675Z","shell.execute_reply.started":"2024-04-10T14:42:36.329264Z","shell.execute_reply":"2024-04-10T14:42:36.337586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Load the datasets**","metadata":{}},{"cell_type":"code","source":"train_dataset = pd.read_csv(\"/kaggle/input/recipe-for-rating-predict-food-ratings-using-ml/train.csv\")\ntest_dataset = pd.read_csv(\"/kaggle/input/recipe-for-rating-predict-food-ratings-using-ml/test.csv\")\nsample_dataset = pd.read_csv(\"/kaggle/input/recipe-for-rating-predict-food-ratings-using-ml/sample.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:36.340556Z","iopub.execute_input":"2024-04-10T14:42:36.341044Z","iopub.status.idle":"2024-04-10T14:42:36.496399Z","shell.execute_reply.started":"2024-04-10T14:42:36.341007Z","shell.execute_reply":"2024-04-10T14:42:36.495442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data samples and statistics**","metadata":{}},{"cell_type":"code","source":"train_dataset.duplicated().sum(), test_dataset.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:36.499699Z","iopub.execute_input":"2024-04-10T14:42:36.500899Z","iopub.status.idle":"2024-04-10T14:42:36.546984Z","shell.execute_reply.started":"2024-04-10T14:42:36.500852Z","shell.execute_reply":"2024-04-10T14:42:36.545873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.shape, test_dataset.shape, sample_dataset.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:36.548310Z","iopub.execute_input":"2024-04-10T14:42:36.549004Z","iopub.status.idle":"2024-04-10T14:42:36.557593Z","shell.execute_reply.started":"2024-04-10T14:42:36.548970Z","shell.execute_reply":"2024-04-10T14:42:36.556283Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:36.559528Z","iopub.execute_input":"2024-04-10T14:42:36.559968Z","iopub.status.idle":"2024-04-10T14:42:36.579560Z","shell.execute_reply.started":"2024-04-10T14:42:36.559927Z","shell.execute_reply":"2024-04-10T14:42:36.578338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:36.581422Z","iopub.execute_input":"2024-04-10T14:42:36.581915Z","iopub.status.idle":"2024-04-10T14:42:36.598382Z","shell.execute_reply.started":"2024-04-10T14:42:36.581874Z","shell.execute_reply":"2024-04-10T14:42:36.597294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_dataset.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:36.600164Z","iopub.execute_input":"2024-04-10T14:42:36.600593Z","iopub.status.idle":"2024-04-10T14:42:36.612921Z","shell.execute_reply.started":"2024-04-10T14:42:36.600554Z","shell.execute_reply":"2024-04-10T14:42:36.611584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.describe()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:36.614642Z","iopub.execute_input":"2024-04-10T14:42:36.615092Z","iopub.status.idle":"2024-04-10T14:42:36.660239Z","shell.execute_reply.started":"2024-04-10T14:42:36.615053Z","shell.execute_reply":"2024-04-10T14:42:36.659153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset.describe()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:36.661579Z","iopub.execute_input":"2024-04-10T14:42:36.662467Z","iopub.status.idle":"2024-04-10T14:42:36.704567Z","shell.execute_reply.started":"2024-04-10T14:42:36.662377Z","shell.execute_reply":"2024-04-10T14:42:36.703471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Correlation graph between 'RecipeNumber' and 'ThumbsUpCount'**\n* Here is a scatter plot between 'RecipeNumber' and 'ThumbsUpCount', with 'RecipeNumber' on the x-axis and 'ThumbsUpCount' on the y-axis. The scatter plot will also include histograms of the individual distributions along the axes.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nfrom scipy.stats import pearsonr\n\n# Create a jointplot\nsns.jointplot(x='RecipeNumber', y='ThumbsUpCount', data=train_dataset, kind='scatter', color='skyblue')\n\n# Calculate the Pearson correlation coefficient\ncorr_coefficient, _ = pearsonr(train_dataset['RecipeNumber'], train_dataset['ThumbsUpCount'])\n\n# Annotate the plot with the Pearson correlation coefficient\nplt.annotate(f'Pearson Correlation: {corr_coefficient:.2f}', xy=(0.5, 0.9), xycoords='axes fraction', ha='center', fontsize=12)\n\n# Show the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:36.705914Z","iopub.execute_input":"2024-04-10T14:42:36.706974Z","iopub.status.idle":"2024-04-10T14:42:37.488925Z","shell.execute_reply.started":"2024-04-10T14:42:36.706932Z","shell.execute_reply":"2024-04-10T14:42:37.488102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Correlation graph between 'RecipeNumber' and 'ThumbsDownCount'**\n* Here is a scatter plot between 'RecipeNumber' and 'ThumbsDownCount', with 'RecipeNumber' on the x-axis and 'ThumbsDownCount' on the y-axis. The scatter plot will also include histograms of the individual distributions along the axes.","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nfrom scipy.stats import pearsonr\n\n# Create a jointplot\nsns.jointplot(x='RecipeNumber', y='ThumbsDownCount', data=train_dataset, kind='scatter', color='skyblue')\n\n# Calculate the Pearson correlation coefficient\ncorr_coefficient, _ = pearsonr(train_dataset['RecipeNumber'], train_dataset['ThumbsDownCount'])\n\n# Annotate the plot with the Pearson correlation coefficient\nplt.annotate(f'Pearson Correlation: {corr_coefficient:.2f}', xy=(0.5, 0.9), xycoords='axes fraction', ha='center', fontsize=12)\n\n# Show the plot\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:37.490176Z","iopub.execute_input":"2024-04-10T14:42:37.490941Z","iopub.status.idle":"2024-04-10T14:42:38.248608Z","shell.execute_reply.started":"2024-04-10T14:42:37.490907Z","shell.execute_reply":"2024-04-10T14:42:38.247321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:38.253266Z","iopub.execute_input":"2024-04-10T14:42:38.253631Z","iopub.status.idle":"2024-04-10T14:42:38.271681Z","shell.execute_reply.started":"2024-04-10T14:42:38.253599Z","shell.execute_reply":"2024-04-10T14:42:38.270351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:38.273142Z","iopub.execute_input":"2024-04-10T14:42:38.273542Z","iopub.status.idle":"2024-04-10T14:42:38.288339Z","shell.execute_reply.started":"2024-04-10T14:42:38.273510Z","shell.execute_reply":"2024-04-10T14:42:38.286829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature engineering/extraction**","metadata":{}},{"cell_type":"code","source":"train_dataset.isna().any()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:38.289704Z","iopub.execute_input":"2024-04-10T14:42:38.290043Z","iopub.status.idle":"2024-04-10T14:42:38.311351Z","shell.execute_reply.started":"2024-04-10T14:42:38.290013Z","shell.execute_reply":"2024-04-10T14:42:38.309415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:38.313923Z","iopub.execute_input":"2024-04-10T14:42:38.314892Z","iopub.status.idle":"2024-04-10T14:42:38.330862Z","shell.execute_reply.started":"2024-04-10T14:42:38.314844Z","shell.execute_reply":"2024-04-10T14:42:38.329930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we assume that there is a direct mapping between Rating and Recipe_Review, and therefore we fill the empty Recipe_Review fields with the Review sentiments mapped to ratings in the rating_mapping dictionary.","metadata":{}},{"cell_type":"code","source":"def fill_null_reviews(train_dataset):\n\n  rating_mapping = {5: 'GREAT',4:'GOOD',2:'BORING',0:'VERY BAD', 1: 'BAD', 3: 'OK'}\n  train_dataset.loc[train_dataset['Recipe_Review'].isna(), 'Recipe_Review'] = train_dataset['Rating'].map(rating_mapping)\n  return train_dataset\n\nfill_null_reviews(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:38.332336Z","iopub.execute_input":"2024-04-10T14:42:38.332823Z","iopub.status.idle":"2024-04-10T14:42:38.357800Z","shell.execute_reply.started":"2024-04-10T14:42:38.332790Z","shell.execute_reply":"2024-04-10T14:42:38.356657Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:38.359010Z","iopub.execute_input":"2024-04-10T14:42:38.359337Z","iopub.status.idle":"2024-04-10T14:42:38.377963Z","shell.execute_reply.started":"2024-04-10T14:42:38.359308Z","shell.execute_reply":"2024-04-10T14:42:38.376945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:38.379422Z","iopub.execute_input":"2024-04-10T14:42:38.380069Z","iopub.status.idle":"2024-04-10T14:42:38.392563Z","shell.execute_reply.started":"2024-04-10T14:42:38.380027Z","shell.execute_reply":"2024-04-10T14:42:38.391510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset.isin(['?']).sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:38.393819Z","iopub.execute_input":"2024-04-10T14:42:38.394205Z","iopub.status.idle":"2024-04-10T14:42:38.426039Z","shell.execute_reply.started":"2024-04-10T14:42:38.394167Z","shell.execute_reply":"2024-04-10T14:42:38.424937Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset.isin(['?']).sum()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:38.427472Z","iopub.execute_input":"2024-04-10T14:42:38.429441Z","iopub.status.idle":"2024-04-10T14:42:38.445385Z","shell.execute_reply.started":"2024-04-10T14:42:38.429395Z","shell.execute_reply":"2024-04-10T14:42:38.444299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset['UserReputation'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:38.446619Z","iopub.execute_input":"2024-04-10T14:42:38.446997Z","iopub.status.idle":"2024-04-10T14:42:38.455159Z","shell.execute_reply.started":"2024-04-10T14:42:38.446967Z","shell.execute_reply":"2024-04-10T14:42:38.454104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the box plot we can see that most of the users have a UserReputation around 0-100.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.boxplot(x='UserReputation', data=train_dataset)\nplt.xlabel('User Reputation')\nplt.ylabel('count')\nplt.title('Box plot of User Reputation')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:38.456436Z","iopub.execute_input":"2024-04-10T14:42:38.456792Z","iopub.status.idle":"2024-04-10T14:42:38.659846Z","shell.execute_reply.started":"2024-04-10T14:42:38.456760Z","shell.execute_reply":"2024-04-10T14:42:38.658748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset['Rating'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:38.662769Z","iopub.execute_input":"2024-04-10T14:42:38.663127Z","iopub.status.idle":"2024-04-10T14:42:38.672732Z","shell.execute_reply.started":"2024-04-10T14:42:38.663096Z","shell.execute_reply":"2024-04-10T14:42:38.671502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the bar graph we can see that most recipes (10371 recipes) have received a Rating of 5, followed by 0 and then 4.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 5))\nsns.countplot(x='Rating', data=train_dataset, palette='coolwarm')\nplt.xlabel('Rating')\nplt.ylabel('Count')\nplt.title('Distribution of Ratings')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:38.673934Z","iopub.execute_input":"2024-04-10T14:42:38.674384Z","iopub.status.idle":"2024-04-10T14:42:38.926310Z","shell.execute_reply.started":"2024-04-10T14:42:38.674324Z","shell.execute_reply":"2024-04-10T14:42:38.925124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset['BestScore'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:38.927649Z","iopub.execute_input":"2024-04-10T14:42:38.928009Z","iopub.status.idle":"2024-04-10T14:42:38.936346Z","shell.execute_reply.started":"2024-04-10T14:42:38.927979Z","shell.execute_reply":"2024-04-10T14:42:38.935382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From the histogram we can see that most recipes (10506 recipes) have received the BestScore of 100, followed by a BestScore of 193. ","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.hist(train_dataset['BestScore'], bins=20, color='skyblue', edgecolor='black')\nplt.xlabel('Best Score')\nplt.ylabel('Frequency')\nplt.title('Histogram of Best Score')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:38.938916Z","iopub.execute_input":"2024-04-10T14:42:38.939344Z","iopub.status.idle":"2024-04-10T14:42:39.220818Z","shell.execute_reply.started":"2024-04-10T14:42:38.939304Z","shell.execute_reply":"2024-04-10T14:42:39.219464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:39.222631Z","iopub.execute_input":"2024-04-10T14:42:39.223036Z","iopub.status.idle":"2024-04-10T14:42:39.228569Z","shell.execute_reply.started":"2024-04-10T14:42:39.223002Z","shell.execute_reply":"2024-04-10T14:42:39.227168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Calculating the correlation matrix between the selected columns of the DataFrame train_dataset and then visualizing the correlations using a heatmap. \n\nThe highest correlation is between BestScore and ThumbsUpCount with a correlation coefficient of 0.69.\n\nWhereas the lowest correlation is between ID and RecipeNumber with a correlation coefficient of -0.49.","metadata":{}},{"cell_type":"code","source":"correlation = train_dataset[['ID', 'RecipeNumber','RecipeCode', 'UserReputation', 'CreationTimestamp', 'ReplyCount',\n       'ThumbsUpCount', 'ThumbsDownCount', 'Rating', 'BestScore']].corr()\nplt.figure(figsize=(12,7))\nsns.heatmap(correlation,annot=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:39.230229Z","iopub.execute_input":"2024-04-10T14:42:39.231397Z","iopub.status.idle":"2024-04-10T14:42:39.937514Z","shell.execute_reply.started":"2024-04-10T14:42:39.231358Z","shell.execute_reply":"2024-04-10T14:42:39.936667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:39.938675Z","iopub.execute_input":"2024-04-10T14:42:39.939621Z","iopub.status.idle":"2024-04-10T14:42:39.945024Z","shell.execute_reply.started":"2024-04-10T14:42:39.939587Z","shell.execute_reply":"2024-04-10T14:42:39.943787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_features = train_dataset.select_dtypes(['object']).columns\nnum_features = train_dataset.select_dtypes(['int','float']).columns","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:39.946583Z","iopub.execute_input":"2024-04-10T14:42:39.946968Z","iopub.status.idle":"2024-04-10T14:42:39.958608Z","shell.execute_reply.started":"2024-04-10T14:42:39.946937Z","shell.execute_reply":"2024-04-10T14:42:39.957488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cat_features","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:39.960263Z","iopub.execute_input":"2024-04-10T14:42:39.960641Z","iopub.status.idle":"2024-04-10T14:42:39.970863Z","shell.execute_reply.started":"2024-04-10T14:42:39.960609Z","shell.execute_reply":"2024-04-10T14:42:39.969688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_features","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:39.972683Z","iopub.execute_input":"2024-04-10T14:42:39.973501Z","iopub.status.idle":"2024-04-10T14:42:39.982098Z","shell.execute_reply.started":"2024-04-10T14:42:39.973467Z","shell.execute_reply":"2024-04-10T14:42:39.980960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Plotting all numerical attributes with histogram plot for quick examination.**\n* Features are at different scales.\n* Features have different distributions -\n * A few are tail heavy. e.g. ID, BestScore\n * A few have a single mode. e.g. UserReputation, CreationTimestamp, ReplyCount, ThumbsUpCount, ThumbsDownCount","metadata":{}},{"cell_type":"code","source":"plot_data = train_dataset.drop(columns=['RecipeName', 'CommentID', 'UserID','UserName', 'Recipe_Review','Rating'])\n# Create subplots for histograms\nfig, axes = plt.subplots(3, 3, figsize=(16, 16))  # 3 rows and 3 columns for 9 features\n\n# Loop through the columns and create histograms\nfor i, column in enumerate(plot_data.columns):\n    row, col = divmod(i, 3)\n    ax = axes[row, col]\n    ax.hist(plot_data[column], bins=20, color='skyblue', alpha=0.7)\n    ax.set_title(column)\n    ax.set_xlabel(column)\n    ax.set_ylabel(\"Frequency\")\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:39.983290Z","iopub.execute_input":"2024-04-10T14:42:39.984146Z","iopub.status.idle":"2024-04-10T14:42:42.493588Z","shell.execute_reply.started":"2024-04-10T14:42:39.984113Z","shell.execute_reply":"2024-04-10T14:42:42.492517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a preprocessing pipeline\nnumeric_transformer = Pipeline(steps=[\n    ('scaler', MinMaxScaler())])\n\ncategorical_transformer = Pipeline(steps=[\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:42.494855Z","iopub.execute_input":"2024-04-10T14:42:42.495234Z","iopub.status.idle":"2024-04-10T14:42:42.500982Z","shell.execute_reply.started":"2024-04-10T14:42:42.495206Z","shell.execute_reply":"2024-04-10T14:42:42.499673Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating a CT preprocessor\nCT = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, num_features),\n        ('cat', categorical_transformer, cat_features)])","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:42.502609Z","iopub.execute_input":"2024-04-10T14:42:42.502964Z","iopub.status.idle":"2024-04-10T14:42:42.515951Z","shell.execute_reply.started":"2024-04-10T14:42:42.502935Z","shell.execute_reply":"2024-04-10T14:42:42.514957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit and transform the data and convert it into a dataframe\ntrain_dataset_transformed = pd.DataFrame(CT.fit_transform(train_dataset))","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:42.517384Z","iopub.execute_input":"2024-04-10T14:42:42.517762Z","iopub.status.idle":"2024-04-10T14:42:43.533252Z","shell.execute_reply.started":"2024-04-10T14:42:42.517697Z","shell.execute_reply":"2024-04-10T14:42:43.532051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset_transformed.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:43.534590Z","iopub.execute_input":"2024-04-10T14:42:43.534938Z","iopub.status.idle":"2024-04-10T14:42:43.547387Z","shell.execute_reply.started":"2024-04-10T14:42:43.534908Z","shell.execute_reply":"2024-04-10T14:42:43.546141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import re\n# from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n# from sklearn.feature_extraction.text import TfidfVectorizer\n\n# # Define function to clean and preprocess text\n# def preprocess_text(text):\n#     # Convert text to lowercase\n#     text = text.lower()\n    \n#     # Remove HTML tags\n#     text = re.sub('<[^<]+?>', '', text)\n    \n#     # Remove non-alphanumeric characters and extra whitespaces\n#     text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n    \n#     # Remove stopwords\n#     stop_words = set(ENGLISH_STOP_WORDS)\n#     tokens = text.split()\n#     tokens = [token for token in tokens if token not in stop_words]\n#     text = ' '.join(tokens)\n    \n#     return text\n\n# # Apply preprocessing to the Recipe_Review column\n# train_dataset['Recipe_Review'] = train_dataset['Recipe_Review'].apply(preprocess_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:43.565949Z","iopub.execute_input":"2024-04-10T14:42:43.566293Z","iopub.status.idle":"2024-04-10T14:42:43.571176Z","shell.execute_reply.started":"2024-04-10T14:42:43.566265Z","shell.execute_reply":"2024-04-10T14:42:43.570038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = train_dataset.drop(columns='Rating', axis=1)\ny = train_dataset['Rating']\nX.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:43.572976Z","iopub.execute_input":"2024-04-10T14:42:43.573400Z","iopub.status.idle":"2024-04-10T14:42:43.595641Z","shell.execute_reply.started":"2024-04-10T14:42:43.573361Z","shell.execute_reply":"2024-04-10T14:42:43.594540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:43.597005Z","iopub.execute_input":"2024-04-10T14:42:43.597870Z","iopub.status.idle":"2024-04-10T14:42:43.610202Z","shell.execute_reply.started":"2024-04-10T14:42:43.597826Z","shell.execute_reply":"2024-04-10T14:42:43.608969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TF-IDF(Term Frequency-Inverse Document Frequency) Vectorizer and Count Vectorizer are popular techniques used in natural language processing (NLP) to convert text data into numerical representations suitable for machine learning algorithms.","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:43.611603Z","iopub.execute_input":"2024-04-10T14:42:43.612051Z","iopub.status.idle":"2024-04-10T14:42:43.620191Z","shell.execute_reply.started":"2024-04-10T14:42:43.612009Z","shell.execute_reply":"2024-04-10T14:42:43.619187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here, we initialize a CountVectorizer object named vec_username to convert the 'UserName' column into a matrix of token counts. It analyzes the text data to determine the unique words (tokens) present in the 'UserName' column and assigns an index to each unique word.","metadata":{}},{"cell_type":"code","source":"vec_username = CountVectorizer()\nvec_username.fit(train_dataset['UserName'].values)\n\nX_train_username = vec_username.transform(train_dataset['UserName'].values)\nX_test_username = vec_username.transform(test_dataset['UserName'].values)\n\nprint(X_train_username.shape, y.shape)\nprint(X_test_username.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:43.621859Z","iopub.execute_input":"2024-04-10T14:42:43.623047Z","iopub.status.idle":"2024-04-10T14:42:43.780312Z","shell.execute_reply.started":"2024-04-10T14:42:43.623003Z","shell.execute_reply":"2024-04-10T14:42:43.779149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This code is similar to the previous one but applied to the 'RecipeName' column instead of the 'UserName' column. It analyzes the text data to determine the unique words (tokens) present in the 'RecipeName' column and assigns an index to each unique word.","metadata":{}},{"cell_type":"code","source":"vec = CountVectorizer()\nvec.fit(train_dataset['RecipeName'].values)\n\nX_train_recipe = vec.transform(train_dataset['RecipeName'].values)\nX_test_recipe = vec.transform(test_dataset['RecipeName'].values)\n\nprint(X_train_recipe.shape, y.shape)\nprint(X_test_recipe.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:43.781768Z","iopub.execute_input":"2024-04-10T14:42:43.782164Z","iopub.status.idle":"2024-04-10T14:42:43.957898Z","shell.execute_reply.started":"2024-04-10T14:42:43.782124Z","shell.execute_reply":"2024-04-10T14:42:43.956628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we transform the 'Recipe_Review' column of the training dataset into a matrix of TF-IDF features ie. words that appear most frequently and are most important in the document. \n\nThis code prepares the text data in the 'Recipe_Review' column for machine learning models by converting it into a numerical representation using TF-IDF features, which can then be used for training and testing classification models.","metadata":{}},{"cell_type":"code","source":"vec_rr = TfidfVectorizer(min_df=5,ngram_range=(1,4), max_features=10000 )\nvec_rr.fit(train_dataset['Recipe_Review'].values)\n\nX_train_rr = vec_rr.transform(train_dataset['Recipe_Review'].values)\nX_test_rr = vec_rr.transform(test_dataset['Recipe_Review'].values)\n\nprint(X_train_rr.shape, y.shape)\nprint(X_test_rr.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:43.959159Z","iopub.execute_input":"2024-04-10T14:42:43.959566Z","iopub.status.idle":"2024-04-10T14:42:51.658490Z","shell.execute_reply.started":"2024-04-10T14:42:43.959527Z","shell.execute_reply":"2024-04-10T14:42:51.657347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train_rr.columns","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:51.659996Z","iopub.execute_input":"2024-04-10T14:42:51.660329Z","iopub.status.idle":"2024-04-10T14:42:51.664063Z","shell.execute_reply.started":"2024-04-10T14:42:51.660301Z","shell.execute_reply":"2024-04-10T14:42:51.663041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.feature_selection import SelectKBest, chi2\n\n# # Create SelectKBest object\n# selector = SelectKBest(score_func=chi2, k=800)\n\n# # Fit selector to training data\n# selector.fit(X_train_rr, y)\n\n# # Transform training and testing data\n# X_train_rr = selector.transform(X_train_rr)\n# X_test_rr = selector.transform(X_test_rr)\n\n# print(X_train_rr.shape, y.shape)\n# print(X_test_rr.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:51.665531Z","iopub.execute_input":"2024-04-10T14:42:51.665914Z","iopub.status.idle":"2024-04-10T14:42:51.677996Z","shell.execute_reply.started":"2024-04-10T14:42:51.665884Z","shell.execute_reply":"2024-04-10T14:42:51.676849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Horizontally stacking sparse matrices X_train_username, X_train_recipe, and X_train_rr into a single sparse matrix X_tr for the training data.\n\nSimilarly stacking X_test_username, X_test_recipe, and X_test_rr into X_te.","metadata":{}},{"cell_type":"code","source":"from scipy.sparse import hstack\n\nX_tr = hstack((X_train_username, X_train_recipe, X_train_rr))\nX_te = hstack((X_test_username, X_test_recipe, X_test_rr))\n\nprint(X_tr.shape, y.shape)\nprint(X_te.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:51.679482Z","iopub.execute_input":"2024-04-10T14:42:51.680173Z","iopub.status.idle":"2024-04-10T14:42:51.703831Z","shell.execute_reply.started":"2024-04-10T14:42:51.680139Z","shell.execute_reply":"2024-04-10T14:42:51.702637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_tr, X_te","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:42:51.704960Z","iopub.execute_input":"2024-04-10T14:42:51.705382Z","iopub.status.idle":"2024-04-10T14:42:51.712569Z","shell.execute_reply.started":"2024-04-10T14:42:51.705354Z","shell.execute_reply":"2024-04-10T14:42:51.711426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Dummy Classifier**\n**Score: 0.76066**","metadata":{}},{"cell_type":"code","source":"# df = pd.read_csv('/kaggle/input/recipe-for-rating-predict-food-ratings-using-ml/train.csv')\n# X = df.drop(\"Rating\", axis = 1)\n# y = df['Rating']\n\n# from sklearn.dummy import DummyClassifier\n# model = DummyClassifier(strategy = \"most_frequent\").fit(X,y)\n\n# X_test = pd.read_csv('/kaggle/input/recipe-for-rating-predict-food-ratings-using-ml/test.csv')\n# y_pred = model.predict(X_test)\n\n# submission = pd.DataFrame({ 'ID': range(1,4547),\n#                             'total_amount': y_pred})\n\n# submission.to_csv('submission.csv', index = False) # converting it to csv file","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Random Forrest Classifier**\n**Score: 0.77298**","metadata":{}},{"cell_type":"code","source":"# from sklearn.ensemble import RandomForestClassifier\n# r=RandomForestClassifier()\n# r.fit(X_tr, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_pred = r.predict(X_tr)\n# y_test_pred=r.predict(X_te)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **CART: Decision Tree Classifier**\n**Score: 0.76022**","metadata":{}},{"cell_type":"code","source":"# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.model_selection import GridSearchCV\n\n# # Create Decision Tree classifier\n# cart = DecisionTreeClassifier()\n\n# # Define hyperparameters and their values to tune\n# param_grid = {\n#     'criterion': ['gini', 'entropy'],    # Splitting criterion\n#     'max_depth': [None, 10, 20, 30],     # Maximum depth of the tree\n#     'min_samples_split': [2, 5, 10],     # Minimum number of samples required to split an internal node\n#     'min_samples_leaf': [1, 2, 4],       # Minimum number of samples required to be at a leaf node\n#     'max_features': ['auto', 'sqrt', 'log2']  # Number of features to consider when looking for the best split\n# }\n\n# # Perform grid search cross-validation\n# grid_search = GridSearchCV(estimator=cart, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n\n# # Fit the grid search to the training data\n# grid_search.fit(X_tr, y)\n\n# # Print the best hyperparameters found\n# print(\"Best hyperparameters:\", grid_search.best_params_)\n\n# # Use the best model found by GridSearchCV\n# best_cart = grid_search.best_estimator_\n\n# # Make predictions on the training and testing sets using the best model\n# y_pred_train = best_cart.predict(X_tr)\n# y_pred_test = best_cart.predict(X_te)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Bagging**\n**Score: 0.77562**","metadata":{}},{"cell_type":"code","source":"# from sklearn.ensemble import BaggingClassifier\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.model_selection import RandomizedSearchCV\n# from scipy.stats import uniform, randint\n\n# # Create base classifier (Decision Tree in this case)\n# base_classifier = DecisionTreeClassifier()\n\n# # Create Bagging Classifier\n# bagging_classifier = BaggingClassifier(base_estimator=base_classifier)\n\n# # Define hyperparameter distributions\n# param_dist = {\n#     'n_estimators': randint(10, 100),        # Randomly sample number of estimators\n#     'max_samples': uniform(0.5, 0.5),        # Uniform distribution for max samples\n#     'max_features': uniform(0.5, 0.5),       # Uniform distribution for max features\n#     'bootstrap': [True, False]\n# }\n\n# # Perform randomized search cross-validation\n# random_search_bagging = RandomizedSearchCV(estimator=bagging_classifier, param_distributions=param_dist, n_iter=10, cv=3, scoring='accuracy', n_jobs=-1, random_state=42)\n\n# # Fit the randomized search to the training data\n# random_search_bagging.fit(X_tr, y)\n\n# # Print the best hyperparameters found\n# print(\"Best hyperparameters for Bagging:\", random_search_bagging.best_params_)\n# # Best hyperparameters for Bagging: {'bootstrap': True, 'max_features': 0.5102922471479012, 'max_samples': 0.9849549260809971, 'n_estimators': 39}\n\n# # Use the best model found by RandomizedSearchCV\n# best_bagging = random_search_bagging.best_estimator_\n\n# # Make predictions on the training and testing sets using the best model\n# y_pred_train = best_bagging.predict(X_tr)\n# y_pred_test = best_bagging.predict(X_te)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Boosting**\n**Score: 0.68741**","metadata":{}},{"cell_type":"code","source":"# from sklearn.ensemble import AdaBoostClassifier\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.model_selection import GridSearchCV\n\n# # Create base classifier (Decision Tree in this case)\n# base_classifier = DecisionTreeClassifier()\n\n# # Create AdaBoost Classifier\n# adaboost_classifier = AdaBoostClassifier(base_classifier)\n\n# # Define hyperparameters and their values to tune\n# param_grid = {\n#     'n_estimators': [50, 100, 200],  # Number of weak learners\n#     'learning_rate': [0.1, 0.5, 1.0],  # Weight applied to each weak learner\n#     'algorithm': ['SAMME', 'SAMME.R']  # Algorithm used for boosting\n# }\n\n# # Perform grid search cross-validation\n# grid_search_adaboost = GridSearchCV(estimator=adaboost_classifier, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n\n# # Fit the grid search to the training data\n# grid_search_adaboost.fit(X_tr, y)\n\n# # Print the best hyperparameters found\n# print(\"Best hyperparameters for AdaBoost:\", grid_search_adaboost.best_params_)\n# # Best hyperparameters for AdaBoost: {'algorithm': 'SAMME', 'learning_rate': 1.0, 'n_estimators': 200}\n\n\n# # Use the best model found by GridSearchCV for AdaBoost\n# best_adaboost = grid_search_adaboost.best_estimator_\n\n# # Make predictions on the training and testing sets using the best AdaBoost model\n# y_pred_train = best_adaboost.predict(X_tr)\n# y_pred_test = best_adaboost.predict(X_te)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **2nd Highest Score: MLP Classifier with Random Search**\n**Score: 0.78970**","metadata":{}},{"cell_type":"markdown","source":"MLP **(Multi-Layer Perceptron)** Classifier is extremely useful in datasets which have a combination of **numerical and categorical features**. In this project, the prediction of Rating is heavily dependent on the categorical feature - RecipeReview, which is based on the **user's sentiments**. Also, MLP classifier (being a **neural network**) is extremely competent in handling **non-linear relationships**. Here, there could be a non-linear relationship between UserReputatipn, ThumbsUpCount, ThumbsDownCount and RecipeReview which could non-linearly affect the Ratings.","metadata":{}},{"cell_type":"code","source":"# from sklearn.neural_network import MLPClassifier\n# from sklearn.model_selection import RandomizedSearchCV\n# from scipy.stats import randint\n\n# # Create MLP classifier\n# mlp = MLPClassifier()\n\n# # Define hyperparameters and their values to tune\n# param_dist = {\n#     'hidden_layer_sizes': [(50,), (100,), (150,)],  # The number of neurons in each hidden layer.\n#     'activation': ['relu', 'tanh'],  # The function applied to the output of each neuron in the hidden layers.\n#     'solver': ['adam'],  # The optimization algorithm used to update the weights of the connections.\n#     'alpha': [0.0001, 0.001],  # Regularization Parameter: alpha(L2 regularization) to prevent overfitting.\n#     'learning_rate': ['constant']  # Determines the step size during weight updates.\n# }\n# # Neurons in hidden layers usually apply non-linear activation functions such as -\n# # ReLU (Rectified Linear Unit), tanh (Hyperbolic Tangent), or sigmoid. They introduce non-linearity to the output\n# # of individual neurons to learn complex relationships and patterns in the data.\n\n# n_iter_search = 20\n\n# # Perform randomized search cross-validation\n# random_search_mlp = RandomizedSearchCV(estimator=mlp, param_distributions=param_dist, n_iter=n_iter_search, cv=5, scoring='accuracy', n_jobs=-1)\n\n# # Fit the randomized search to the training data\n# random_search_mlp.fit(X_tr, y)\n\n# # Print the best hyperparameters found\n# print(\"Best hyperparameters for MLP:\", random_search_mlp.best_params_)\n# # Best hyperparameters for MLP: {'solver': 'adam', 'learning_rate': 'constant', 'hidden_layer_sizes': (150,), 'alpha': 0.001, 'activation': 'relu'}\n\n# # Use the best model found by RandomizedSearchCV for MLP\n# best_mlp = random_search_mlp.best_estimator_\n\n# # Make predictions on the training and testing sets using the best MLP model\n# y_pred_train = best_mlp.predict(X_tr)\n# y_pred_test = best_mlp.predict(X_te)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-08T14:36:21.530005Z","iopub.execute_input":"2024-04-08T14:36:21.530568Z","iopub.status.idle":"2024-04-08T16:55:21.329684Z","shell.execute_reply.started":"2024-04-08T14:36:21.530506Z","shell.execute_reply":"2024-04-08T16:55:21.326771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **1st Highest Score: MLP Classifier with Grid Search**\n**Score: 0.79014**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.neural_network import MLPClassifier\nfrom scipy.stats import randint\n\n# Define hyperparameters and their values to tune\nparam_grid = {\n    'hidden_layer_sizes': [(150,)],  # The number of neurons in each hidden layer.\n    'activation': ['relu'],  # The function applied to the output of each neuron in the hidden layers.\n    'solver': ['adam'],  # The optimization algorithm used to update the weights of the connections.\n    'alpha': [0.001],  # Regularization Parameter: alpha(L2 regularization) to prevent overfitting.\n    'learning_rate': ['constant']  # Determines the step size during weight updates.\n}\n\n# Create MLP classifier\nmlp = MLPClassifier()\n\n# Perform grid search cross-validation\ngrid_search_mlp = GridSearchCV(estimator=mlp, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n\n# Fit the grid search to the training data\ngrid_search_mlp.fit(X_tr, y)\n\n# Print the best hyperparameters found\nprint(\"Best hyperparameters for MLP:\", grid_search_mlp.best_params_)\n# Best hyperparameters for MLP: {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (150,), 'learning_rate': 'constant', 'solver': 'adam'}\n\n# Use the best model found by GridSearchCV for MLP\nbest_mlp = grid_search_mlp.best_estimator_\n\n# Make predictions on the training and testing sets using the best MLP model\ny_pred_train = best_mlp.predict(X_tr)\ny_pred_test = best_mlp.predict(X_te)","metadata":{"execution":{"iopub.status.busy":"2024-04-09T12:53:47.936638Z","iopub.execute_input":"2024-04-09T12:53:47.937202Z","iopub.status.idle":"2024-04-09T13:21:48.213483Z","shell.execute_reply.started":"2024-04-09T12:53:47.937160Z","shell.execute_reply":"2024-04-09T13:21:48.211625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **KNN**\n**Score: 0.76572**","metadata":{}},{"cell_type":"code","source":"# from sklearn.neighbors import KNeighborsClassifier\n# from sklearn.model_selection import GridSearchCV\n\n# # Create KNN classifier\n# knn = KNeighborsClassifier()\n\n# # Define hyperparameters and their values to tune\n# param_grid = {\n#     'n_neighbors': [3, 5, 7, 10],           # Number of neighbors\n#     'weights': ['uniform', 'distance'],     # Weight function used in prediction\n#     'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],  # Algorithm used to compute the nearest neighbors\n#     'p': [1, 2]                              # Power parameter for the Minkowski metric\n# }\n\n# # Perform grid search cross-validation\n# grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n\n# # Fit the grid search to the training data\n# grid_search.fit(X_tr, y)\n\n# # Print the best hyperparameters found\n# print(\"Best hyperparameters:\", grid_search.best_params_)\n\n# # Use the best model found by GridSearchCV\n# best_knn = grid_search.best_estimator_\n\n# # Make predictions on the training and testing sets using the best model\n# y_pred_train = best_knn.predict(X_tr)\n# y_pred_test = best_knn.predict(X_te)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **4th Highest Score: SVM**\n**Score: 0.7853**","metadata":{}},{"cell_type":"code","source":"# from sklearn.svm import SVC\n# from sklearn.model_selection import GridSearchCV\n\n# # Create SVC classifier\n# svm = SVC()\n\n# # Define hyperparameters and their values to tune\n# param_grid = {\n#     'C': [10],               # Regularization parameter \n# #                                             High value of C gives small margin and large classification error\n# #                                             Small value of C gives large margin and small error.\n#     'kernel': ['rbf'],  # It computes the dot product between the input features and is \n# #                                             suitable for linearly separable datasets.\n#     'gamma': ['scale'],            # Kernel coefficient for 'rbf', 'poly', 'sigmoid'\n#     'degree': [2]\n# }\n# # It computes the dot product between the input features and is suitable for linearly separable datasets.\n# # The polynomial kernel: polynomial of the dot product of their feature vectors.It introduces nonlinearity\n# #                    into the decision boundary, allowing SVMs to capture more complex relationships in the data.\n# # The RBF kernel, also known as the Gaussian kernel, computes the similarity between samples based on the \n# #                      Gaussian (radial basis) function.\n    \n    \n# # Perform grid search cross-validation\n# grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n\n# # Fit the grid search to the training data\n# grid_search.fit(X_tr, y)\n\n# # Print the best hyperparameters found\n# print(\"Best hyperparameters:\", grid_search.best_params_)\n# Best hyperparameters: {'C': 10, 'degree': 2, 'gamma': 'scale', 'kernel': 'rbf'}\n\n# # Use the best model found by GridSearchCV\n# best_svm = grid_search.best_estimator_\n\n# # Make predictions on the training and testing sets using the best model\n# y_pred_train = best_svm.predict(X_tr)\n# y_pred_test = best_svm.predict(X_te)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **XGBClassifier**\n**Score: 0.7809**","metadata":{}},{"cell_type":"code","source":"# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.model_selection import train_test_split, GridSearchCV\n# from xgboost import XGBClassifier\n\n# # Fill missing values\n# train_dataset = train_dataset.fillna('')\n\n# # Split the dataset\n# X = train_dataset[\"Recipe_Review\"]\n# y = train_dataset[\"Rating\"]\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# # Vectorize the text data\n# vectorizer = TfidfVectorizer()\n\n# X_train_vectorized = vectorizer.fit_transform(X_train)\n# X_test_vectorized = vectorizer.transform(X_test)\n\n# # Define the parameter grid for hyperparameter tuning\n# param_grid = {\n#     'n_estimators': [100, 200, 300],\n#     'learning_rate': [0.01, 0.1, 0.2],\n#     'max_depth': [3, 4, 5]\n# }\n\n# # Initialize the XGBoost classifier\n# xgb_model = XGBClassifier()\n\n# # Perform grid search cross-validation\n# grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, scoring='accuracy')\n\n# # Fit the grid search to the training data\n# # grid_search.fit(X_train_vectorized, y_train)\n# grid_search.fit(X_tr, y)\n\n# # Print the best hyperparameters found\n# print(\"Best hyperparameters:\", grid_search.best_params_)\n\n# # Use the best model found by GridSearchCV\n# best_xgb_model = grid_search.best_estimator_\n\n# # Make predictions on the training and testing sets using the best model\n# # y_pred_train = best_xgb_model.predict(X_train_vectorized)\n# # y_pred_test = best_xgb_model.predict(X_test_vectorized)\n# y_pred_train = best_xgb_model.predict(X_tr)\n# y_pred_test = best_xgb_model.predict(X_te)\n\n# # xgb_model.fit(X_train, y_train)\n# # y_pred = xgb_model.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **3rd Highest Score : Logistic Regression**\n**Score: 0.78860**","metadata":{}},{"cell_type":"code","source":"# from sklearn.linear_model import LogisticRegression\n# from sklearn.model_selection import GridSearchCV\n\n# LR = LogisticRegression(max_iter=1000)\n\n# # Define hyperparameters and their values to tune\n# param_grid = {\n#     'penalty': ['l1', 'l2'],  # Regularization penalty\n#     'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Inverse of regularization strength\n#     'solver': ['liblinear', 'saga']  # Optimization algorithm\n# }\n\n# # Perform grid search cross-validation\n# grid_search = GridSearchCV(estimator=LR, param_grid=param_grid, cv=5, scoring='accuracy')\n\n# # Fit the grid search to the training data\n# grid_search.fit(X_tr, y)\n\n# # Print the best hyperparameters found\n\n# print(\"Best hyperparameters:\", grid_search.best_params_)\n\n# # Use the best model found by GridSearchCV\n# best_LR = grid_search.best_estimator_\n\n# # Make predictions on the training and testing sets using the best model\n# y_pred_train = best_LR.predict(X_tr)\n# y_pred_test = best_LR.predict(X_te)\n\n# # LR.fit(X_tr, y)\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **LGBM Classifier**\n**Score: 0.78332**","metadata":{}},{"cell_type":"code","source":"# from lightgbm import LGBMClassifier\n\n# # Initialize the LightGBM classifier\n# lgbm = LGBMClassifier()\n\n# # Fit the classifier to the training data\n# lgbm.fit(X_tr, y)\n\n# # Make predictions on the training and testing sets\n# y_pred_train = lgbm.predict(X_tr)\n# y_pred_test = lgbm.predict(X_te)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from lightgbm import LGBMClassifier\n# from sklearn.model_selection import GridSearchCV\n\n# lgbm = LGBMClassifier()\n\n# # Define hyperparameters and their values to tune\n# param_grid = {\n#     'learning_rate': np.arange(0.01,0.2),\n#     'n_estimators': range(50,250),\n#     'max_depth': [3, 5, 10, 20, 30, 50],\n#     'min_child_samples': [3, 5, 10, 20, 30, 50],\n#     'subsample': [0.2, 0.4, 0.6, 0.8, 0.9],\n#     'colsample_bytree': [0.2, 0.4, 0.6, 0.8, 0.9],\n#     'reg_alpha': [0.0, 0.2, 0.5, 0.7, 1.0],\n#     'reg_lambda': [0.0, 0.2, 0.5, 0.7, 1.0],\n#     'n_jobs': [-1]\n# }\n\n# # param_grid = {\n# #         'learning_rate': np.arange( 0.01, 0.2),\n# #         'n_estimators': range(50, 250),\n# #         'max_depth': range(3, 50),\n# #         'min_child_samples': range(3, 50),\n# #         'subsample': np.arange(0.2, 0.9),\n# #         'colsample_bytree': np.arange(0.2, 0.9),\n# #         'reg_alpha': np.arange(0.0, 1.0),\n# #         'reg_lambda': np.arange(0.0, 1.0),\n# #         'n_jobs': [-1]\n# # }\n\n# # Perform grid search cross-validation\n# grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, cv=5, scoring='accuracy')\n\n# # Fit the grid search to the training data\n# grid_search.fit(X_tr, y)\n\n# # Print the best hyperparameters found\n\n# print(\"Best hyperparameters:\", grid_search.best_params_)\n\n# # Use the best model found by GridSearchCV\n# best_LGBM = grid_search.best_estimator_\n\n# # Make predictions on the training and testing sets using the best model\n# y_pred_train = best_LGBM.predict(X_tr)\n# y_pred_test = best_LGBM.predict(X_te)\n\n# # LR.fit(X_tr, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Modified LGBM\n\n# from lightgbm import LGBMClassifier\n# from sklearn.model_selection import GridSearchCV\n\n# lgbm = LGBMClassifier()\n\n# # Define hyperparameters and their values to tune\n# param_grid = {\n#     'learning_rate': [0.01, 0.1, 0.2],  # Narrowed down the range\n#     'n_estimators': [50, 100, 150],       # Narrowed down the range\n#     'max_depth': [3, 5, 10],              # Limited the values\n#     'min_child_samples': [3, 5, 10],       # Limited the values\n#     'subsample': [0.2, 0.6, 0.9],         # Limited the values\n#     'colsample_bytree': [0.2, 0.6, 0.9],  # Limited the values\n#     'reg_alpha': [0.0, 0.5, 1.0],          # Limited the values\n#     'reg_lambda': [0.0, 0.5, 1.0],         # Limited the values\n#     'n_jobs': [-1]\n# }\n\n# # Perform grid search cross-validation\n# grid_search = GridSearchCV(estimator=lgbm, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n\n# # Fit the grid search to the training data\n# grid_search.fit(X_tr, y)\n\n# # Print the best hyperparameters found\n# print(\"Best hyperparameters:\", grid_search.best_params_)\n\n# # Use the best model found by GridSearchCV\n# best_LGBM = grid_search.best_estimator_\n\n# # Make predictions on the training and testing sets using the best model\n# y_pred_train = best_LGBM.predict(X_tr)\n# y_pred_test = best_LGBM.predict(X_te)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Alternate Code:\n# #Score: 0.78332\n\n# from lightgbm import LGBMClassifier\n# from sklearn.model_selection import RandomizedSearchCV\n# from scipy.stats import randint, uniform\n\n# lgbm = LGBMClassifier()\n\n# # Define hyperparameters and their values to tune\n# param_dist = {\n#     'learning_rate': uniform(0.01, 0.19),  # Uniform distribution between 0.01 and 0.2\n#     'n_estimators': randint(50, 250),       # Discrete uniform distribution between 50 and 250\n#     'max_depth': randint(3, 51),            # Discrete uniform distribution between 3 and 50\n#     'min_child_samples': randint(3, 51),     # Discrete uniform distribution between 3 and 50\n#     'subsample': uniform(0.2, 0.7),         # Uniform distribution between 0.2 and 0.9\n#     'colsample_bytree': uniform(0.2, 0.7),  # Uniform distribution between 0.2 and 0.9\n#     'reg_alpha': uniform(0.0, 1.0),          # Uniform distribution between 0.0 and 1.0\n#     'reg_lambda': uniform(0.0, 1.0),         # Uniform distribution between 0.0 and 1.0\n#     'n_jobs': [-1]\n# }\n\n# # Perform randomized search cross-validation\n# random_search = RandomizedSearchCV(estimator=lgbm, param_distributions=param_dist, n_iter=100, cv=5, scoring='accuracy', random_state=42)\n\n# # Fit the randomized search to the training data\n# random_search.fit(X_tr, y)\n\n# # Print the best hyperparameters found\n# print(\"Best hyperparameters:\", random_search.best_params_)\n\n# # Use the best model found by RandomizedSearchCV\n# best_LGBM = random_search.best_estimator_\n\n# # Make predictions on the training and testing sets using the best model\n# y_pred_train = best_LGBM.predict(X_tr)\n# y_pred_test = best_LGBM.predict(X_te)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Stacking Classifier**","metadata":{}},{"cell_type":"code","source":"# from sklearn.ensemble import StackingClassifier\n# from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n# from sklearn.neural_network import MLPClassifier\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.svm import SVC\n# from scipy.stats import randint\n\n# # Define base classifiers\n# mlp = MLPClassifier()\n# lr = LogisticRegression(max_iter=1000)\n# svm = SVC()\n\n# # Define hyperparameters and their values to tune for each base classifier\n# param_dist_mlp = {\n#     'hidden_layer_sizes': [(150,)],\n#     'activation': ['relu'],\n#     'solver': ['adam'],\n#     'alpha': [0.001],\n#     'learning_rate': ['constant']\n# }\n\n# param_dist_lr = {\n#     'penalty': ['l1', 'l2'],\n#     'C': [0.001, 0.01, 0.1, 1, 10, 100],\n#     'solver': ['liblinear', 'saga']\n# }\n\n# param_dist_svm = {\n#     'C': [10],\n#     'degree': [2],\n#     'kernel': ['rbf'],\n#     'gamma': ['scale']\n# }\n\n# # Define the Stacking Classifier\n# estimators = [\n#     ('mlp', MLPClassifier()),\n#     ('lr', LogisticRegression(max_iter=1000)),\n#     ('svm', SVC())\n# ]\n# stacking_classifier = StackingClassifier(estimators=estimators, final_estimator=MLPClassifier())\n\n# # Define hyperparameters and their values to tune for the stacking classifier\n# param_dist_stacking = {\n#     'final_estimator__hidden_layer_sizes': [(100,)],\n#     'final_estimator__activation': ['relu'],\n#     'final_estimator__solver': ['adam'],\n#     'final_estimator__alpha': [0.0001],\n#     'final_estimator__learning_rate': ['constant']\n# }\n\n# # Define StratifiedKFold for cross-validation\n# cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n\n# # Perform Randomized Search CV for Stacking Classifier\n# random_search_stacking = RandomizedSearchCV(estimator=stacking_classifier, param_distributions=param_dist_stacking, n_iter=10, cv=cv, scoring='accuracy', n_jobs=-1)\n\n# # Fit the randomized search to the training data\n# random_search_stacking.fit(X_tr, y)\n\n# # Print the best hyperparameters found\n# print(\"Best hyperparameters for Stacking Classifier:\", random_search_stacking.best_params_)\n# # Best hyperparameters for Stacking Classifier: {'final_estimator__solver': 'adam', 'final_estimator__learning_rate': 'constant', 'final_estimator__hidden_layer_sizes': (100,), 'final_estimator__alpha': 0.0001, 'final_estimator__activation': 'relu'}\n\n# # Use the best model found by RandomizedSearchCV for Stacking Classifier\n# best_stacking = random_search_stacking.best_estimator_\n\n# # Make predictions on the training and testing sets using the best Stacking Classifier model\n# y_pred_train_stacking = best_stacking.predict(X_tr)\n# y_pred_test_stacking = best_stacking.predict(X_te)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-10T13:27:27.686689Z","iopub.execute_input":"2024-04-10T13:27:27.687149Z","iopub.status.idle":"2024-04-10T14:31:04.185631Z","shell.execute_reply.started":"2024-04-10T13:27:27.687116Z","shell.execute_reply":"2024-04-10T14:31:04.184007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Submission Code**","metadata":{}},{"cell_type":"code","source":"ID=[]\nfor i in range (1,len(y_pred_test)+1):\n    ID.append(i)\nfinal_pred = pd.DataFrame({\"ID\":ID,\"total_amount\":y_pred_test})\nfinal_pred.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T14:41:18.398820Z","iopub.execute_input":"2024-04-10T14:41:18.399195Z","iopub.status.idle":"2024-04-10T14:41:18.426969Z","shell.execute_reply.started":"2024-04-10T14:41:18.399166Z","shell.execute_reply":"2024-04-10T14:41:18.425467Z"},"trusted":true},"execution_count":null,"outputs":[]}]}